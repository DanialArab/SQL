{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a966b64b",
   "metadata": {},
   "source": [
    "**To leverage Spark SQL API to make SQL queries**:\n",
    "\n",
    "I defined two functions: one is spark_df_reader to connect to the MySQL database through jdbc Driver and read the tables as a dictionary called dfs whose keys are the names of the tables and the values are the corresponding dataframes. The second function is temporary_view_registrator to register the Spark dataframes (one per table in the database) as a temporary view to be able to pass in direct SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8db85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/danial/spark-3.4.0-bin-hadoop3')\n",
    "import pyspark \n",
    "import os\n",
    "password = os.environ.get('MYSQL_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7951f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/29 22:06:22 WARN Utils: Your hostname, danial-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/07/29 22:06:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/29 22:06:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MySQL Session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c044a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_df_reader(database_name, table_names ):\n",
    "    \n",
    "    # table_names is a list of table names in the database that I want to connect to \n",
    "    \n",
    "    mysql_url = f\"jdbc:mysql://localhost:3306/{database_name}\"\n",
    "    \n",
    "    mysql_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "    }\n",
    "    \n",
    "    dfs = {}\n",
    "    for one_table in table_names:\n",
    "        \n",
    "        df = spark.read.jdbc(url=mysql_url, table=one_table, properties=mysql_properties)\n",
    "\n",
    "        dfs[one_table] = df\n",
    "        \n",
    "    return dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b4dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to register my Spark dataframes (one per table in the database) as a temporary view to be able to pass in direct SQL queries \n",
    "\n",
    "def temporary_view_registrator(dfs):\n",
    "    \n",
    "    # dfs is a dictionary whose keys are the name of tables and values are the corresponding dfs\n",
    "    \n",
    "    tem_views = []\n",
    "    for one_table in list(dfs.keys()):\n",
    "        dfs[one_table].createOrReplaceTempView(f\"{one_table}\")\n",
    "        tem_views.append(f\"{one_table}\")\n",
    "        \n",
    "    return None # this functions returns nothing but creates temporary views with the same name as the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dfa80f",
   "metadata": {},
   "source": [
    "### 1757 Recyclable and Low Fat Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f33d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1757', ['Products'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ece80872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|product_id|low_fats|recyclable|\n",
      "+----------+--------+----------+\n",
      "|         0|       Y|         N|\n",
      "|         1|       Y|         Y|\n",
      "|         2|       N|         Y|\n",
      "|         3|       Y|         Y|\n",
      "|         4|       N|         N|\n",
      "+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs['Products'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cccc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf15f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT product_id\n",
    "FROM Products\n",
    "WHERE low_fats = 'y' AND recyclable = 'Y'\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d7de9",
   "metadata": {},
   "source": [
    "### 1350 Students With Invalid Departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5fb03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1350', ['Departments', 'Students'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33bfae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  4|Jasmine|\n",
      "|  7| Daiana|\n",
      "|  2|   John|\n",
      "|  3|  Steve|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    s.id, s.name\n",
    "FROM Students s \n",
    "LEFT JOIN Departments d\n",
    "    ON d.id = s.department_id\n",
    "WHERE d.id IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ad11584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  2|   John|\n",
      "|  4|Jasmine|\n",
      "|  3|  Steve|\n",
      "|  7| Daiana|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# better solution\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT id, name\n",
    "FROM Students\n",
    "WHERE department_id NOT IN (SELECT id FROM Departments)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260bbf80",
   "metadata": {},
   "source": [
    "### 1303 Find the Team Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15700856",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1303', ['Employee'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fe6c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|employee_id|team_size|\n",
      "+-----------+---------+\n",
      "|          4|        1|\n",
      "|          1|        3|\n",
      "|          2|        3|\n",
      "|          3|        3|\n",
      "|          5|        2|\n",
      "|          6|        2|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    employee_id,\n",
    "    COUNT(employee_id) OVER(PARTITION BY team_id)AS team_size\n",
    "FROM Employee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21ff6b",
   "metadata": {},
   "source": [
    "### 1741 Find Total Time Spent by Each Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f06c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1741', ['Employees'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "696c322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+\n",
      "|       day|emp_id|total_time|\n",
      "+----------+------+----------+\n",
      "|2020-12-09|     2|        27|\n",
      "|2020-11-28|     1|       173|\n",
      "|2020-11-28|     2|        30|\n",
      "|2020-12-03|     1|        41|\n",
      "+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DISTINCT event_day AS day,\n",
    "    emp_id,\n",
    "    SUM(out_time - in_time) OVER(PARTITION BY emp_id, event_day) AS total_time    \n",
    "FROM Employees\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afb07b",
   "metadata": {},
   "source": [
    "### 1821 Find Customers With Positive Revenue this Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee05c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1821', ['Customers'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0d87eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT customer_id\n",
    "FROM Customers\n",
    "WHERE revenue > 0 AND year = 2021\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885133da",
   "metadata": {},
   "source": [
    "### 1571 Warehouse Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932529ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1571', ['Warehouse', 'Products'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebee0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|warehouse_name|volume|\n",
      "+--------------+------+\n",
      "|      LCHouse2| 20250|\n",
      "|      LCHouse1| 12250|\n",
      "|      LCHouse3|   800|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DISTINCT name AS warehouse_name,\n",
    "    SUM(Width * Length * Height * units) OVER(PARTITION BY name) AS volume\n",
    "FROM Warehouse w\n",
    "JOIN Products p\n",
    "    USING (product_id)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a76d8",
   "metadata": {},
   "source": [
    "### 2356 Number of Unique Subjects Taught by Each Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ac90e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2356', ['Teacher'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4b5303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|teacher_id|cnt|\n",
      "+----------+---+\n",
      "|         1|  2|\n",
      "|         2|  4|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    teacher_id,\n",
    "    COUNT(DISTINCT subject_id) AS cnt\n",
    "FROM Teacher\n",
    "GROUP BY teacher_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6c160",
   "metadata": {},
   "source": [
    "### 1693 Daily Leads and Partners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3383314",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1693', ['DailySales'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db46576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+---------------+\n",
      "|   date_id|make_name|unique_leads|unique_partners|\n",
      "+----------+---------+------------+---------------+\n",
      "|2020-12-07|    honda|           3|              2|\n",
      "|2020-12-08|   toyota|           2|              3|\n",
      "|2020-12-08|    honda|           2|              2|\n",
      "|2020-12-07|   toyota|           1|              2|\n",
      "+----------+---------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_id, \n",
    "    make_name,\n",
    "    COUNT(DISTINCT lead_id) AS unique_leads,\n",
    "    COUNT(DISTINCT partner_id) AS unique_partners\n",
    "FROM DailySales\n",
    "GROUP BY date_id, make_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44273c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
