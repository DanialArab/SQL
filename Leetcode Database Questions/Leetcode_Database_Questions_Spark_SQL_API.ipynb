{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a966b64b",
   "metadata": {},
   "source": [
    "**To leverage Spark SQL API to make SQL queries**:\n",
    "\n",
    "I defined two functions: one is spark_df_reader to connect to the MySQL database through jdbc Driver and read the tables as a dictionary called dfs whose keys are the names of the tables and the values are the corresponding dataframes. The second function is temporary_view_registrator to register the Spark dataframes (one per table in the database) as a temporary view to be able to pass in direct SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8db85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/danial/spark-3.4.0-bin-hadoop3')\n",
    "import pyspark \n",
    "import os\n",
    "password = os.environ.get('MYSQL_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7951f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MySQL Session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c044a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_df_reader(database_name, table_names ):\n",
    "    \n",
    "    # table_names is a list of table names in the database that I want to connect to \n",
    "    \n",
    "    mysql_url = f\"jdbc:mysql://localhost:3306/{database_name}\"\n",
    "    \n",
    "    mysql_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "    }\n",
    "    \n",
    "    dfs = {}\n",
    "    for one_table in table_names:\n",
    "        \n",
    "        df = spark.read.jdbc(url=mysql_url, table=one_table, properties=mysql_properties)\n",
    "\n",
    "        dfs[one_table] = df\n",
    "        \n",
    "    return dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b4dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to register my Spark dataframes (one per table in the database) as a temporary view to be able to pass in direct SQL queries \n",
    "\n",
    "def temporary_view_registrator(dfs):\n",
    "    \n",
    "    # dfs is a dictionary whose keys are the name of tables and values are the corresponding dfs\n",
    "    \n",
    "    tem_views = []\n",
    "    for one_table in list(dfs.keys()):\n",
    "        dfs[one_table].createOrReplaceTempView(f\"{one_table}\")\n",
    "        tem_views.append(f\"{one_table}\")\n",
    "        \n",
    "    return None # this functions returns nothing but creates temporary views with the same name as the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dfa80f",
   "metadata": {},
   "source": [
    "### 1757 Recyclable and Low Fat Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f33d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1757', ['Products'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ece80872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|product_id|low_fats|recyclable|\n",
      "+----------+--------+----------+\n",
      "|         0|       Y|         N|\n",
      "|         1|       Y|         Y|\n",
      "|         2|       N|         Y|\n",
      "|         3|       Y|         Y|\n",
      "|         4|       N|         N|\n",
      "+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs['Products'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cccc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf15f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT product_id\n",
    "FROM Products\n",
    "WHERE low_fats = 'y' AND recyclable = 'Y'\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d7de9",
   "metadata": {},
   "source": [
    "### 1350 Students With Invalid Departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5fb03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1350', ['Departments', 'Students'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33bfae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  4|Jasmine|\n",
      "|  7| Daiana|\n",
      "|  2|   John|\n",
      "|  3|  Steve|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    s.id, s.name\n",
    "FROM Students s \n",
    "LEFT JOIN Departments d\n",
    "    ON d.id = s.department_id\n",
    "WHERE d.id IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ad11584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  2|   John|\n",
      "|  4|Jasmine|\n",
      "|  3|  Steve|\n",
      "|  7| Daiana|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# better solution\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT id, name\n",
    "FROM Students\n",
    "WHERE department_id NOT IN (SELECT id FROM Departments)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260bbf80",
   "metadata": {},
   "source": [
    "### 1303 Find the Team Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15700856",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1303', ['Employee'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fe6c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|employee_id|team_size|\n",
      "+-----------+---------+\n",
      "|          4|        1|\n",
      "|          1|        3|\n",
      "|          2|        3|\n",
      "|          3|        3|\n",
      "|          5|        2|\n",
      "|          6|        2|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    employee_id,\n",
    "    COUNT(employee_id) OVER(PARTITION BY team_id)AS team_size\n",
    "FROM Employee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21ff6b",
   "metadata": {},
   "source": [
    "### 1741 Find Total Time Spent by Each Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f06c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1741', ['Employees'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "696c322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+\n",
      "|       day|emp_id|total_time|\n",
      "+----------+------+----------+\n",
      "|2020-12-09|     2|        27|\n",
      "|2020-11-28|     1|       173|\n",
      "|2020-11-28|     2|        30|\n",
      "|2020-12-03|     1|        41|\n",
      "+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DISTINCT event_day AS day,\n",
    "    emp_id,\n",
    "    SUM(out_time - in_time) OVER(PARTITION BY emp_id, event_day) AS total_time    \n",
    "FROM Employees\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afb07b",
   "metadata": {},
   "source": [
    "### 1821 Find Customers With Positive Revenue this Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee05c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1821', ['Customers'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0d87eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT customer_id\n",
    "FROM Customers\n",
    "WHERE revenue > 0 AND year = 2021\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885133da",
   "metadata": {},
   "source": [
    "### 1571 Warehouse Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932529ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1571', ['Warehouse', 'Products'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebee0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|warehouse_name|volume|\n",
      "+--------------+------+\n",
      "|      LCHouse2| 20250|\n",
      "|      LCHouse1| 12250|\n",
      "|      LCHouse3|   800|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DISTINCT name AS warehouse_name,\n",
    "    SUM(Width * Length * Height * units) OVER(PARTITION BY name) AS volume\n",
    "FROM Warehouse w\n",
    "JOIN Products p\n",
    "    USING (product_id)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a76d8",
   "metadata": {},
   "source": [
    "### 2356 Number of Unique Subjects Taught by Each Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ac90e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2356', ['Teacher'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4b5303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|teacher_id|cnt|\n",
      "+----------+---+\n",
      "|         1|  2|\n",
      "|         2|  4|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    teacher_id,\n",
    "    COUNT(DISTINCT subject_id) AS cnt\n",
    "FROM Teacher\n",
    "GROUP BY teacher_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6c160",
   "metadata": {},
   "source": [
    "### 1693 Daily Leads and Partners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3383314",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1693', ['DailySales'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db46576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+---------------+\n",
      "|   date_id|make_name|unique_leads|unique_partners|\n",
      "+----------+---------+------------+---------------+\n",
      "|2020-12-07|    honda|           3|              2|\n",
      "|2020-12-08|   toyota|           2|              3|\n",
      "|2020-12-08|    honda|           2|              2|\n",
      "|2020-12-07|   toyota|           1|              2|\n",
      "+----------+---------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_id, \n",
    "    make_name,\n",
    "    COUNT(DISTINCT lead_id) AS unique_leads,\n",
    "    COUNT(DISTINCT partner_id) AS unique_partners\n",
    "FROM DailySales\n",
    "GROUP BY date_id, make_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbdcdc",
   "metadata": {},
   "source": [
    "### 2339 All the Matches of the League"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e19c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2339', ['Teams'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c82c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|  home_team|  away_team|\n",
      "+-----------+-----------+\n",
      "|Leetcode FC|    Ahly SC|\n",
      "|Leetcode FC|Real Madrid|\n",
      "|    Ahly SC|Leetcode FC|\n",
      "|    Ahly SC|Real Madrid|\n",
      "|Real Madrid|Leetcode FC|\n",
      "|Real Madrid|    Ahly SC|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT t.team_name AS home_team, tt.team_name AS away_team \n",
    "FROM Teams t\n",
    "CROSS JOIN Teams tt \n",
    "WHERE t.team_name <> tt.team_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2333c3e",
   "metadata": {},
   "source": [
    "### 1683 Invalid Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae6e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1683', ['Tweets'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea8e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT tweet_id\n",
    "FROM Tweets\n",
    "WHERE LENGTH(content) > 15\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c78842",
   "metadata": {},
   "source": [
    "### 1853 Convert Date Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "719eb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1853', ['Days'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "748711ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|            day|\n",
      "+---------------+\n",
      "|Tue, 4 12, 2022|\n",
      "| Mon, 8 9, 2021|\n",
      "|Fri, 6 26, 2020|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DATE_FORMAT(day, 'E, M d, y') AS day\n",
    "FROM Days\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbf8de",
   "metadata": {},
   "source": [
    "### 1378 Replace Employee ID With The Unique Identifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255dbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1378', ['Employees', 'EmployeeUNI'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686f1d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     null|   Alice|\n",
      "|        1|Jonathan|\n",
      "|     null|     Bob|\n",
      "|        3| Winston|\n",
      "|        2|    Meir|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    ee.unique_id,\n",
    "    e.name\n",
    "FROM Employees e\n",
    "LEFT JOIN EmployeeUNI ee\n",
    "    USING (id)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92f5c5",
   "metadata": {},
   "source": [
    "### 1623 All Valid Triplets That Can Represent a Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad22e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1623', ['SchoolA', 'SchoolB', 'SchoolC'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1a21374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|member_A|member_B|member_C|\n",
      "+--------+--------+--------+\n",
      "|   Alice|     Tom|   Jerry|\n",
      "|     Bob|     Tom|   Alice|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    a.student_name AS member_A,\n",
    "    b.student_name AS member_B,\n",
    "    c.student_name AS member_C\n",
    "FROM SchoolA a\n",
    "JOIN SchoolB b\n",
    "    ON a.student_id <> b.student_id AND a.student_name <> b.student_name \n",
    "JOIN SchoolC c\n",
    "    ON a.student_id <> c.student_id AND a.student_name <> c.student_name  AND\n",
    "       c.student_id <> b.student_id AND c.student_name <> b.student_name \n",
    "\"\"\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04799dd",
   "metadata": {},
   "source": [
    "### 1587 Bank Account Summary II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b7ee0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1587', ['Users', 'Transactions'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f43cad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| name|balance|\n",
      "+-----+-------+\n",
      "|Alice|  11000|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DISTINCT u.name,\n",
    "    SUM(amount) AS balance\n",
    "FROM Users u\n",
    "JOIN Transactions t\n",
    "    USING (account)\n",
    "GROUP BY u.name\n",
    "HAVING balance > 10000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b8d01",
   "metadata": {},
   "source": [
    "### 2026 Low-Quality Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca43d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2026', ['Problems'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "980686e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|problem_id|\n",
      "+----------+\n",
      "|         7|\n",
      "|        10|\n",
      "|        11|\n",
      "|        13|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    problem_id\n",
    "FROM Problems\n",
    "WHERE likes / (likes + dislikes) * 100 < 60 \n",
    "GROUP BY problem_id\n",
    "ORDER BY problem_id\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eac1d6",
   "metadata": {},
   "source": [
    "### 627 Swap Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbee490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_627', ['Salary'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01d7a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|sex|salary|\n",
      "+---+----+---+------+\n",
      "|  1|   A|  f|  2500|\n",
      "|  2|   B|  m|  1500|\n",
      "|  3|   C|  f|  5500|\n",
      "|  4|   D|  m|   500|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT id, name, CASE\n",
    "                    WHEN sex = 'f' THEN 'm' ELSE 'f'\n",
    "                END AS sex, salary\n",
    "FROM Salary\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f7a9a",
   "metadata": {},
   "source": [
    "### 1421 NPV Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06d2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1421', ['NPV', 'Queries'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b45edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|year|npv|\n",
      "+---+----+---+\n",
      "|  7|2018|  0|\n",
      "|  7|2020| 30|\n",
      "|  2|2008|121|\n",
      "|  1|2019|113|\n",
      "|  7|2019|  0|\n",
      "| 13|2019| 40|\n",
      "|  3|2009| 21|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    q.id, q.year, IFNULL(n.npv, 0) AS npv\n",
    "FROM Queries q\n",
    "LEFT JOIN NPV n\n",
    "USING (id, year)\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257ea41",
   "metadata": {},
   "source": [
    "### 1777 Product's Price for Each Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f49f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1777', ['Products'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee079f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+\n",
      "|product_id|store1|store2|store3|\n",
      "+----------+------+------+------+\n",
      "|         1|    70|  null|    80|\n",
      "|         0|    95|   100|   105|\n",
      "+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    product_id, \n",
    "    MAX(IF (store = 'store1', price, null)) AS store1,\n",
    "    MAX(IF (store = 'store2', price, null)) AS store2,\n",
    "    MAX(IF (store = 'store3', price, null)) AS store3\n",
    "FROM Products\n",
    "GROUP BY product_id \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a71dae",
   "metadata": {},
   "source": [
    "### 1565 Unique Orders and Customers Per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b73bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1565', ['Orders'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c654917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------+\n",
      "|  month|order_count|customer_count|\n",
      "+-------+-----------+--------------+\n",
      "|2020-12|          2|             1|\n",
      "|2020-09|          2|             2|\n",
      "|2021-01|          1|             1|\n",
      "|2020-10|          1|             1|\n",
      "+-------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    LEFT(order_date, 7) AS month,\n",
    "    COUNT(DISTINCT order_id) AS order_count,\n",
    "    COUNT(DISTINCT customer_id) AS customer_count\n",
    "FROM Orders\n",
    "WHERE invoice > 20\n",
    "GROUP BY month\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76188d93",
   "metadata": {},
   "source": [
    "### 1173 Immediate Food Delivery I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bb09616",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1173', ['Delivery'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eaaf13da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|immediate_percentage|\n",
      "+--------------------+\n",
      "|               33.33|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "        ROUND(SUM(CASE \n",
    "                    WHEN order_date = customer_pref_delivery_date THEN 1 ELSE 0 END)/COUNT(*) * 100, 2) AS immediate_percentage\n",
    "FROM Delivery\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87204a5d",
   "metadata": {},
   "source": [
    "### 613 Shortest Distance in a Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf66477",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_613 ', ['Point'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb95ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|shortest|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    MIN(ABS(p.x - pp.x)) AS shortest\n",
    "FROM Point p\n",
    "JOIN Point pp\n",
    "    ON p.x <> pp.x\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6600eba",
   "metadata": {},
   "source": [
    "### 2082 The Number of Rich Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b669dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2082', ['Store'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ded5ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|rich_count|\n",
      "+----------+\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT customer_id)AS rich_count\n",
    "FROM Store\n",
    "WHERE amount > 500\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bb0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62d62f",
   "metadata": {},
   "source": [
    "### 1179 Reformat Department Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97ea4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1179', ['Department'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83306ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "| id|Jan_Revenue|Feb_Revenue|Mar_Revenue|Apr_Revenue|May_Revenue|Jun_Revenue|Jul_Revenue|Aug_Revenue|Sep_Revenue|Oct_Revenue|Nov_Revenue|Dec_Revenue|\n",
      "+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|  1|       8000|       7000|       6000|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n",
      "|  3|       null|      10000|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n",
      "|  2|       9000|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n",
      "+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    SUM(IF(month = 'Jan', revenue, NULL)) AS Jan_Revenue,\n",
    "    SUM(IF(month = 'Feb', revenue, NULL)) AS Feb_Revenue,\n",
    "    SUM(IF(month = 'Mar', revenue, NULL)) AS Mar_Revenue,\n",
    "    SUM(IF(month = 'Apr', revenue, NULL)) AS Apr_Revenue,\n",
    "    SUM(IF(month = 'May', revenue, NULL)) AS May_Revenue,\n",
    "    SUM(IF(month = 'Jun', revenue, NULL)) AS Jun_Revenue,\n",
    "    SUM(IF(month = 'Jul', revenue, NULL)) AS Jul_Revenue,\n",
    "    SUM(IF(month = 'Aug', revenue, NULL)) AS Aug_Revenue,\n",
    "    SUM(IF(month = 'Sep', revenue, NULL)) AS Sep_Revenue,\n",
    "    SUM(IF(month = 'Oct', revenue, NULL)) AS Oct_Revenue,\n",
    "    SUM(IF(month = 'Nov', revenue, NULL)) AS Nov_Revenue,\n",
    "    SUM(IF(month = 'Dec', revenue, NULL)) AS Dec_Revenue\n",
    "FROM Department\n",
    "GROUP BY id\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c8f3",
   "metadata": {},
   "source": [
    "### 1581 Customer Who Visited but Did Not Make Any Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e99480",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1581', ['Visits', 'Transactions'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72a4319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|count_no_trans|\n",
      "+-----------+--------------+\n",
      "|         54|             2|\n",
      "|         96|             1|\n",
      "|         30|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT customer_id,\n",
    "        SUM(CASE \n",
    "            WHEN amount IS NULL THEN 1 ELSE 0 END ) AS count_no_trans\n",
    "FROM Visits\n",
    "LEFT JOIN Transactions\n",
    "    USING (visit_id)\n",
    "GROUP BY customer_id\n",
    "HAVING count_no_trans <> 0 \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19619473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
