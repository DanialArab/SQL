{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a966b64b",
   "metadata": {},
   "source": [
    "**To leverage Spark SQL API to make SQL queries**:\n",
    "\n",
    "I defined two functions: one is spark_df_reader to connect to the MySQL database through jdbc Driver and read the tables as a dictionary called dfs whose keys are the names of the tables and the values are the corresponding dataframes. The second function is temporary_view_registrator to register the Spark dataframes (one per table in the database) as a temporary view to be able to pass in direct SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8db85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/danial/spark-3.4.0-bin-hadoop3')\n",
    "import pyspark \n",
    "import os\n",
    "password = os.environ.get('MYSQL_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7951f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MySQL Session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c044a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_df_reader(database_name, table_names ):\n",
    "    \n",
    "    # table_names is a list of table names in the database that I want to connect to \n",
    "    \n",
    "    mysql_url = f\"jdbc:mysql://localhost:3306/{database_name}\"\n",
    "    \n",
    "    mysql_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "    }\n",
    "    \n",
    "    dfs = {}\n",
    "    for one_table in table_names:\n",
    "        \n",
    "        df = spark.read.jdbc(url=mysql_url, table=one_table, properties=mysql_properties)\n",
    "\n",
    "        dfs[one_table] = df\n",
    "        \n",
    "    return dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b4dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to register my Spark dataframes (one per table in the database) as a temporary view to be able to pass in direct SQL queries \n",
    "\n",
    "def temporary_view_registrator(dfs):\n",
    "    \n",
    "    # dfs is a dictionary whose keys are the name of tables and values are the corresponding dfs\n",
    "    \n",
    "    tem_views = []\n",
    "    for one_table in list(dfs.keys()):\n",
    "        dfs[one_table].createOrReplaceTempView(f\"{one_table}\")\n",
    "        tem_views.append(f\"{one_table}\")\n",
    "        \n",
    "    return None # this functions returns nothing but creates temporary views with the same name as the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dfa80f",
   "metadata": {},
   "source": [
    "### 1757 Recyclable and Low Fat Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f33d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1757', ['Products'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ece80872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+\n",
      "|product_id|low_fats|recyclable|\n",
      "+----------+--------+----------+\n",
      "|         0|       Y|         N|\n",
      "|         1|       Y|         Y|\n",
      "|         2|       N|         Y|\n",
      "|         3|       Y|         Y|\n",
      "|         4|       N|         N|\n",
      "+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs['Products'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cccc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf15f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT product_id\n",
    "FROM Products\n",
    "WHERE low_fats = 'y' AND recyclable = 'Y'\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d7de9",
   "metadata": {},
   "source": [
    "### 1350 Students With Invalid Departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5fb03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1350', ['Departments', 'Students'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33bfae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  4|Jasmine|\n",
      "|  7| Daiana|\n",
      "|  2|   John|\n",
      "|  3|  Steve|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    s.id, s.name\n",
    "FROM Students s \n",
    "LEFT JOIN Departments d\n",
    "    ON d.id = s.department_id\n",
    "WHERE d.id IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ad11584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  2|   John|\n",
      "|  4|Jasmine|\n",
      "|  3|  Steve|\n",
      "|  7| Daiana|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# better solution\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT id, name\n",
    "FROM Students\n",
    "WHERE department_id NOT IN (SELECT id FROM Departments)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260bbf80",
   "metadata": {},
   "source": [
    "### 1303 Find the Team Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15700856",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1303', ['Employee'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fe6c0e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|employee_id|team_size|\n",
      "+-----------+---------+\n",
      "|          4|        1|\n",
      "|          1|        3|\n",
      "|          2|        3|\n",
      "|          3|        3|\n",
      "|          5|        2|\n",
      "|          6|        2|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    employee_id,\n",
    "    COUNT(employee_id) OVER(PARTITION BY team_id)AS team_size\n",
    "FROM Employee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21ff6b",
   "metadata": {},
   "source": [
    "### 1741 Find Total Time Spent by Each Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f06c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1741', ['Employees'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "696c322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+\n",
      "|       day|emp_id|total_time|\n",
      "+----------+------+----------+\n",
      "|2020-12-09|     2|        27|\n",
      "|2020-11-28|     1|       173|\n",
      "|2020-11-28|     2|        30|\n",
      "|2020-12-03|     1|        41|\n",
      "+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DISTINCT event_day AS day,\n",
    "    emp_id,\n",
    "    SUM(out_time - in_time) OVER(PARTITION BY emp_id, event_day) AS total_time    \n",
    "FROM Employees\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afb07b",
   "metadata": {},
   "source": [
    "### 1821 Find Customers With Positive Revenue this Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee05c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1821', ['Customers'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0d87eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT customer_id\n",
    "FROM Customers\n",
    "WHERE revenue > 0 AND year = 2021\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885133da",
   "metadata": {},
   "source": [
    "### 1571 Warehouse Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932529ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1571', ['Warehouse', 'Products'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebee0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|warehouse_name|volume|\n",
      "+--------------+------+\n",
      "|      LCHouse2| 20250|\n",
      "|      LCHouse1| 12250|\n",
      "|      LCHouse3|   800|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DISTINCT name AS warehouse_name,\n",
    "    SUM(Width * Length * Height * units) OVER(PARTITION BY name) AS volume\n",
    "FROM Warehouse w\n",
    "JOIN Products p\n",
    "    USING (product_id)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a76d8",
   "metadata": {},
   "source": [
    "### 2356 Number of Unique Subjects Taught by Each Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ac90e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2356', ['Teacher'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4b5303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|teacher_id|cnt|\n",
      "+----------+---+\n",
      "|         1|  2|\n",
      "|         2|  4|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    teacher_id,\n",
    "    COUNT(DISTINCT subject_id) AS cnt\n",
    "FROM Teacher\n",
    "GROUP BY teacher_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6c160",
   "metadata": {},
   "source": [
    "### 1693 Daily Leads and Partners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3383314",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1693', ['DailySales'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db46576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+---------------+\n",
      "|   date_id|make_name|unique_leads|unique_partners|\n",
      "+----------+---------+------------+---------------+\n",
      "|2020-12-07|    honda|           3|              2|\n",
      "|2020-12-08|   toyota|           2|              3|\n",
      "|2020-12-08|    honda|           2|              2|\n",
      "|2020-12-07|   toyota|           1|              2|\n",
      "+----------+---------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_id, \n",
    "    make_name,\n",
    "    COUNT(DISTINCT lead_id) AS unique_leads,\n",
    "    COUNT(DISTINCT partner_id) AS unique_partners\n",
    "FROM DailySales\n",
    "GROUP BY date_id, make_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbdcdc",
   "metadata": {},
   "source": [
    "### 2339 All the Matches of the League"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e19c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2339', ['Teams'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c82c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|  home_team|  away_team|\n",
      "+-----------+-----------+\n",
      "|Leetcode FC|    Ahly SC|\n",
      "|Leetcode FC|Real Madrid|\n",
      "|    Ahly SC|Leetcode FC|\n",
      "|    Ahly SC|Real Madrid|\n",
      "|Real Madrid|Leetcode FC|\n",
      "|Real Madrid|    Ahly SC|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT t.team_name AS home_team, tt.team_name AS away_team \n",
    "FROM Teams t\n",
    "CROSS JOIN Teams tt \n",
    "WHERE t.team_name <> tt.team_name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2333c3e",
   "metadata": {},
   "source": [
    "### 1683 Invalid Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae6e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1683', ['Tweets'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea8e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT tweet_id\n",
    "FROM Tweets\n",
    "WHERE LENGTH(content) > 15\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c78842",
   "metadata": {},
   "source": [
    "### 1853 Convert Date Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "719eb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1853', ['Days'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "748711ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|            day|\n",
      "+---------------+\n",
      "|Tue, 4 12, 2022|\n",
      "| Mon, 8 9, 2021|\n",
      "|Fri, 6 26, 2020|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DATE_FORMAT(day, 'E, M d, y') AS day\n",
    "FROM Days\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbf8de",
   "metadata": {},
   "source": [
    "### 1378 Replace Employee ID With The Unique Identifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255dbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1378', ['Employees', 'EmployeeUNI'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686f1d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     null|   Alice|\n",
      "|        1|Jonathan|\n",
      "|     null|     Bob|\n",
      "|        3| Winston|\n",
      "|        2|    Meir|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    ee.unique_id,\n",
    "    e.name\n",
    "FROM Employees e\n",
    "LEFT JOIN EmployeeUNI ee\n",
    "    USING (id)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92f5c5",
   "metadata": {},
   "source": [
    "### 1623 All Valid Triplets That Can Represent a Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad22e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1623', ['SchoolA', 'SchoolB', 'SchoolC'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1a21374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|member_A|member_B|member_C|\n",
      "+--------+--------+--------+\n",
      "|   Alice|     Tom|   Jerry|\n",
      "|     Bob|     Tom|   Alice|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    a.student_name AS member_A,\n",
    "    b.student_name AS member_B,\n",
    "    c.student_name AS member_C\n",
    "FROM SchoolA a\n",
    "JOIN SchoolB b\n",
    "    ON a.student_id <> b.student_id AND a.student_name <> b.student_name \n",
    "JOIN SchoolC c\n",
    "    ON a.student_id <> c.student_id AND a.student_name <> c.student_name  AND\n",
    "       c.student_id <> b.student_id AND c.student_name <> b.student_name \n",
    "\"\"\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04799dd",
   "metadata": {},
   "source": [
    "### 1587 Bank Account Summary II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b7ee0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1587', ['Users', 'Transactions'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f43cad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| name|balance|\n",
      "+-----+-------+\n",
      "|Alice|  11000|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DISTINCT u.name,\n",
    "    SUM(amount) AS balance\n",
    "FROM Users u\n",
    "JOIN Transactions t\n",
    "    USING (account)\n",
    "GROUP BY u.name\n",
    "HAVING balance > 10000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b8d01",
   "metadata": {},
   "source": [
    "### 2026 Low-Quality Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca43d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2026', ['Problems'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "980686e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|problem_id|\n",
      "+----------+\n",
      "|         7|\n",
      "|        10|\n",
      "|        11|\n",
      "|        13|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    problem_id\n",
    "FROM Problems\n",
    "WHERE likes / (likes + dislikes) * 100 < 60 \n",
    "GROUP BY problem_id\n",
    "ORDER BY problem_id\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eac1d6",
   "metadata": {},
   "source": [
    "### 627 Swap Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbee490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_627', ['Salary'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01d7a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n",
      "| id|name|sex|salary|\n",
      "+---+----+---+------+\n",
      "|  1|   A|  f|  2500|\n",
      "|  2|   B|  m|  1500|\n",
      "|  3|   C|  f|  5500|\n",
      "|  4|   D|  m|   500|\n",
      "+---+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT id, name, CASE\n",
    "                    WHEN sex = 'f' THEN 'm' ELSE 'f'\n",
    "                END AS sex, salary\n",
    "FROM Salary\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f7a9a",
   "metadata": {},
   "source": [
    "### 1421 NPV Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06d2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1421', ['NPV', 'Queries'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b45edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|year|npv|\n",
      "+---+----+---+\n",
      "|  7|2018|  0|\n",
      "|  7|2020| 30|\n",
      "|  2|2008|121|\n",
      "|  1|2019|113|\n",
      "|  7|2019|  0|\n",
      "| 13|2019| 40|\n",
      "|  3|2009| 21|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    q.id, q.year, IFNULL(n.npv, 0) AS npv\n",
    "FROM Queries q\n",
    "LEFT JOIN NPV n\n",
    "USING (id, year)\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257ea41",
   "metadata": {},
   "source": [
    "### 1777 Product's Price for Each Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f49f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1777', ['Products'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee079f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+\n",
      "|product_id|store1|store2|store3|\n",
      "+----------+------+------+------+\n",
      "|         1|    70|  null|    80|\n",
      "|         0|    95|   100|   105|\n",
      "+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    product_id, \n",
    "    MAX(IF (store = 'store1', price, null)) AS store1,\n",
    "    MAX(IF (store = 'store2', price, null)) AS store2,\n",
    "    MAX(IF (store = 'store3', price, null)) AS store3\n",
    "FROM Products\n",
    "GROUP BY product_id \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a71dae",
   "metadata": {},
   "source": [
    "### 1565 Unique Orders and Customers Per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b73bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1565', ['Orders'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c654917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------+\n",
      "|  month|order_count|customer_count|\n",
      "+-------+-----------+--------------+\n",
      "|2020-12|          2|             1|\n",
      "|2020-09|          2|             2|\n",
      "|2021-01|          1|             1|\n",
      "|2020-10|          1|             1|\n",
      "+-------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    LEFT(order_date, 7) AS month,\n",
    "    COUNT(DISTINCT order_id) AS order_count,\n",
    "    COUNT(DISTINCT customer_id) AS customer_count\n",
    "FROM Orders\n",
    "WHERE invoice > 20\n",
    "GROUP BY month\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76188d93",
   "metadata": {},
   "source": [
    "### 1173 Immediate Food Delivery I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bb09616",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1173', ['Delivery'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eaaf13da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|immediate_percentage|\n",
      "+--------------------+\n",
      "|               33.33|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "        ROUND(SUM(CASE \n",
    "                    WHEN order_date = customer_pref_delivery_date THEN 1 ELSE 0 END)/COUNT(*) * 100, 2) AS immediate_percentage\n",
    "FROM Delivery\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87204a5d",
   "metadata": {},
   "source": [
    "### 613 Shortest Distance in a Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf66477",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_613 ', ['Point'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb95ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|shortest|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    MIN(ABS(p.x - pp.x)) AS shortest\n",
    "FROM Point p\n",
    "JOIN Point pp\n",
    "    ON p.x <> pp.x\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6600eba",
   "metadata": {},
   "source": [
    "### 2082 The Number of Rich Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b669dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2082', ['Store'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ded5ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|rich_count|\n",
      "+----------+\n",
      "|         2|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT customer_id)AS rich_count\n",
    "FROM Store\n",
    "WHERE amount > 500\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62d62f",
   "metadata": {},
   "source": [
    "### 1179 Reformat Department Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97ea4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1179', ['Department'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83306ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "| id|Jan_Revenue|Feb_Revenue|Mar_Revenue|Apr_Revenue|May_Revenue|Jun_Revenue|Jul_Revenue|Aug_Revenue|Sep_Revenue|Oct_Revenue|Nov_Revenue|Dec_Revenue|\n",
      "+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|  1|       8000|       7000|       6000|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n",
      "|  3|       null|      10000|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n",
      "|  2|       9000|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n",
      "+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    SUM(IF(month = 'Jan', revenue, NULL)) AS Jan_Revenue,\n",
    "    SUM(IF(month = 'Feb', revenue, NULL)) AS Feb_Revenue,\n",
    "    SUM(IF(month = 'Mar', revenue, NULL)) AS Mar_Revenue,\n",
    "    SUM(IF(month = 'Apr', revenue, NULL)) AS Apr_Revenue,\n",
    "    SUM(IF(month = 'May', revenue, NULL)) AS May_Revenue,\n",
    "    SUM(IF(month = 'Jun', revenue, NULL)) AS Jun_Revenue,\n",
    "    SUM(IF(month = 'Jul', revenue, NULL)) AS Jul_Revenue,\n",
    "    SUM(IF(month = 'Aug', revenue, NULL)) AS Aug_Revenue,\n",
    "    SUM(IF(month = 'Sep', revenue, NULL)) AS Sep_Revenue,\n",
    "    SUM(IF(month = 'Oct', revenue, NULL)) AS Oct_Revenue,\n",
    "    SUM(IF(month = 'Nov', revenue, NULL)) AS Nov_Revenue,\n",
    "    SUM(IF(month = 'Dec', revenue, NULL)) AS Dec_Revenue\n",
    "FROM Department\n",
    "GROUP BY id\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c8f3",
   "metadata": {},
   "source": [
    "### 1581 Customer Who Visited but Did Not Make Any Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e99480",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1581', ['Visits', 'Transactions'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe4646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|count_no_trans|\n",
      "+-----------+--------------+\n",
      "|         54|             2|\n",
      "|         96|             1|\n",
      "|         30|             1|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT customer_id,\n",
    "        SUM(CASE \n",
    "            WHEN amount IS NULL THEN 1 ELSE 0 END ) AS count_no_trans\n",
    "FROM Visits\n",
    "LEFT JOIN Transactions\n",
    "    USING (visit_id)\n",
    "GROUP BY customer_id\n",
    "HAVING count_no_trans <> 0 \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda46dc",
   "metadata": {},
   "source": [
    "### 2377 Sort the Olympic Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe6ce024",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2377 ', ['Olympic'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72192e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------------+-------------+\n",
      "|    country|gold_medals|silver_medals|bronze_medals|\n",
      "+-----------+-----------+-------------+-------------+\n",
      "|      China|         10|           10|           20|\n",
      "|        USA|         10|           10|           20|\n",
      "|     Israel|          2|            2|            3|\n",
      "|      Egypt|          2|            2|            2|\n",
      "|South Sudan|          0|            0|            1|\n",
      "+-----------+-----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM Olympic\n",
    "ORDER BY \n",
    "  gold_medals DESC, \n",
    "  silver_medals DESC, \n",
    "  bronze_medals DESC, \n",
    "  country\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0ac52",
   "metadata": {},
   "source": [
    "### 1484 Group Sold Products By The Date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8462fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1484', ['Activities'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "254405b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "| sell_date|num_sold|            products|\n",
      "+----------+--------+--------------------+\n",
      "|2020-05-30|       3|Basketball, Headp...|\n",
      "|2020-06-01|       2|       Bible, Pencil|\n",
      "|2020-06-02|       1|                Mask|\n",
      "+----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    sell_date, \n",
    "    COUNT(DISTINCT product) AS num_sold, \n",
    "    CONCAT_WS(', ', SORT_ARRAY(COLLECT_LIST(DISTINCT product))) AS products\n",
    "FROM Activities\n",
    "GROUP BY sell_date\n",
    "ORDER BY sell_date  \n",
    "\"\"\").show()\n",
    "\n",
    "\n",
    "# SELECT \n",
    "#     sell_date, \n",
    "#     COUNT(DISTINCT product) AS num_sold, \n",
    "#     GROUP_CONCAT(DISTINCT product) AS products\n",
    "# FROM Activities\n",
    "# GROUP BY sell_date\n",
    "# ORDER BY sell_date \n",
    "\n",
    "# Apache Spark  doesn't natively support the GROUP_CONCAT() function that's available in MySQL. \n",
    "# In Spark SQL, I can achieve similar results using the collect_list() or collect_set() functions \n",
    "# along with the concat_ws() function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb06b607",
   "metadata": {},
   "source": [
    "### 1890 The Latest Login in 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7684370",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1890', ['Logins'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10ac698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|         last_stamp|\n",
      "+-------+-------------------+\n",
      "|      6|2020-06-30 15:06:07|\n",
      "|      8|2020-12-30 00:46:50|\n",
      "|      2|2020-01-16 02:49:50|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    user_id,\n",
    "    MAX(time_stamp) AS last_stamp \n",
    "FROM Logins\n",
    "WHERE YEAR(time_stamp) = 2020\n",
    "GROUP BY user_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3677413",
   "metadata": {},
   "source": [
    "### 1251 Average Selling Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "604f0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1251', ['Prices', 'UnitsSold'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9196d6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|product_id|average_price|\n",
      "+----------+-------------+\n",
      "|         1|         6.96|\n",
      "|         2|        16.96|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    p.product_id,\n",
    "    ROUND(SUM(p.price * u.units) / SUM(units), 2) AS average_price\n",
    "FROM Prices p\n",
    "JOIN UnitsSold u\n",
    "    ON p.product_id = u.product_id AND\n",
    "    purchase_date BETWEEN start_date AND end_date\n",
    "GROUP BY p.product_id\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54f0d1",
   "metadata": {},
   "source": [
    "### 1435 Create a Session Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "698f2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1435', ['Sessions'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62aaf082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "[Stage 0:>                  (0 + 1) / 1][Stage 1:>                  (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       bin|total|\n",
      "+----------+-----+\n",
      "|     [0-5>|    3|\n",
      "|    [5-10>|    1|\n",
      "|   [10-15>|    0|\n",
      "|15 or more|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    '[0-5>' AS bin,\n",
    "    SUM(CASE\n",
    "        WHEN duration/60 >= 0 AND duration/60 < 5 THEN 1 ELSE 0\n",
    "    END) AS total\n",
    "FROM Sessions\n",
    "UNION\n",
    "SELECT \n",
    "    '[5-10>' AS bin,\n",
    "    SUM(CASE\n",
    "        WHEN duration/60 >= 5 AND duration/60 < 10 THEN 1 ELSE 0\n",
    "    END) AS total\n",
    "FROM Sessions\n",
    "UNION\n",
    "SELECT \n",
    "    '[10-15>' AS bin,\n",
    "    SUM(CASE\n",
    "        WHEN duration/60 >= 10 AND duration/60 < 15 THEN 1 ELSE 0\n",
    "    END) AS total\n",
    "FROM Sessions\n",
    "UNION\n",
    "SELECT \n",
    "    '15 or more' AS bin,\n",
    "    SUM(CASE\n",
    "        WHEN duration/60 >= 15 THEN 1 ELSE 0\n",
    "    END) AS total\n",
    "FROM Sessions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6289438",
   "metadata": {},
   "source": [
    "### 1148 Article Views I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0e1c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1148', ['Views'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcda2bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT author_id AS id\n",
    "FROM Views\n",
    "WHERE author_id = viewer_id\n",
    "ORDER BY id \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bff7ca",
   "metadata": {},
   "source": [
    "### 2687. Bikes Last Time Used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffde1eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2687', ['Bikes'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196b8128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "[Stage 0:===========================================================(1 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------------+-------------------+\n",
      "|ride_id|bike_number|         start_time|           end_time|\n",
      "+-------+-----------+-------------------+-------------------+\n",
      "|      1|     W00576|2012-03-25 11:30:00|2012-03-25 12:40:00|\n",
      "|      2|     W00300|2012-03-25 10:30:00|2012-03-25 10:50:00|\n",
      "|      3|     W00455|2012-03-26 14:30:00|2012-03-26 17:40:00|\n",
      "|      4|     W00455|2012-03-25 12:30:00|2012-03-25 13:40:00|\n",
      "|      5|     W00576|2012-03-25 08:10:00|2012-03-25 09:10:00|\n",
      "|      6|     W00576|2012-03-28 02:30:00|2012-03-28 02:50:00|\n",
      "+-------+-----------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM Bikes\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f8822b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|bike_number|           end_time|\n",
      "+-----------+-------------------+\n",
      "|     W00576|2012-03-28 02:50:00|\n",
      "|     W00455|2012-03-26 17:40:00|\n",
      "|     W00300|2012-03-25 10:50:00|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    bike_number,\n",
    "    end_time\n",
    "FROM Bikes\n",
    "WHERE end_time IN (\n",
    "        SELECT MAX(end_time)\n",
    "        FROM Bikes\n",
    "        GROUP BY bike_number\n",
    "\n",
    ")\n",
    "ORDER BY end_time DESC\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbb819",
   "metadata": {},
   "source": [
    "### 175. Combine Two Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0bb2834",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_175', ['Person', 'Address'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adb42bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+--------+\n",
      "|firstName|lastName|         city|   state|\n",
      "+---------+--------+-------------+--------+\n",
      "|     Wang|   Allen|         null|    null|\n",
      "|    Alice|     Bob|New York City|New York|\n",
      "+---------+--------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    firstName,\n",
    "    lastName,\n",
    "    city,\n",
    "    state\n",
    "FROM Person p\n",
    "LEFT JOIN Address a\n",
    "    USING (personId)\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535930e",
   "metadata": {},
   "source": [
    "### 511. Game Play Analysis I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8472164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_511', ['Activity'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1a075e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|player_id|first_login|\n",
      "+---------+-----------+\n",
      "|        1| 2016-03-01|\n",
      "|        3| 2016-03-02|\n",
      "|        2| 2017-06-25|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    player_id,\n",
    "    min(event_date) AS first_login\n",
    "FROM Activity\n",
    "GROUP BY player_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99a6d4",
   "metadata": {},
   "source": [
    "### 1082. Sales Analysis I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2fc767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1082', ['Product', 'Sales'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c57b902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|seller_id|\n",
      "+---------+\n",
      "|        1|\n",
      "|        3|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH cte AS (SELECT \n",
    "    seller_id, SUM(price) AS summ\n",
    "FROM Sales\n",
    "GROUP BY seller_id)\n",
    "\n",
    "SELECT seller_id\n",
    "FROM cte\n",
    "WHERE summ = (\n",
    "    SELECT MAX(summ)\n",
    "    FROM cte\n",
    ")\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e9937",
   "metadata": {},
   "source": [
    "### 577. Employee Bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9beb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_577', ['Employee', 'Bonus'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d673a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|bonus|\n",
      "+----+-----+\n",
      "|John| null|\n",
      "|Brad| null|\n",
      "| Dan|  500|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    e.name,\n",
    "    b.bonus\n",
    "FROM Employee e\n",
    "LEFT JOIN Bonus b\n",
    "    USING (empId)\n",
    "WHERE bonus < 1000 OR bonus IS NULL \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b3ff3",
   "metadata": {},
   "source": [
    "### 2072. The Winner University\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbf28070",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2072', ['NewYork', 'California'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "906be6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|             winner|\n",
      "+-------------------+\n",
      "|New York University|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH cte AS (SELECT \n",
    "    'New York University' AS id,\n",
    "    SUM(\n",
    "        CASE WHEN score >= 90 THEN 1 ELSE 0 END\n",
    "    ) AS num_exc\n",
    "FROM NewYork\n",
    "UNION \n",
    "SELECT \n",
    "    'California University' AS id, \n",
    "    SUM(\n",
    "        CASE WHEN score >= 90 THEN 1 ELSE 0 END\n",
    "    ) AS num_exc\n",
    "FROM California)\n",
    "\n",
    "SELECT \n",
    "    IF(COUNT(id) = 1, id, 'No Winner') AS winner\n",
    "FROM cte\n",
    "WHERE num_exc = (\n",
    "    SELECT MAX(num_exc)\n",
    "    FROM cte    \n",
    ")\n",
    "GROUP BY id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae088ed",
   "metadata": {},
   "source": [
    "### 2837. Total Traveled Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "521c740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_2837', ['Users', 'Rides'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28dfec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+\n",
      "|user_id|   name|traveled distance|\n",
      "+-------+-------+-----------------+\n",
      "|      2|  Avery|              393|\n",
      "|      4|Michael|              416|\n",
      "|     10|Eleanor|                0|\n",
      "|     14|  Ethan|              186|\n",
      "|     17|Addison|              160|\n",
      "+-------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT\n",
    "    user_id,\n",
    "    name,\n",
    "    IFNULL(SUM(distance), 0) AS `traveled distance`\n",
    "FROM Users u\n",
    "LEFT JOIN Rides r\n",
    "    USING (user_id)\n",
    "GROUP BY user_id, name\n",
    "ORDER BY user_id\n",
    "\n",
    "\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06622dd5",
   "metadata": {},
   "source": [
    "### 620. Not Boring Movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "472c529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_620', ['cinema'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1afe746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM cinema\n",
    "WHERE id%2 <> 0 AND description <> 'boring'\n",
    "ORDER BY rating DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d72f26",
   "metadata": {},
   "source": [
    "### 1965. Employees With Missing Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc72097",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1965', ['Employees', 'Salaries'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6799b61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT employee_id\n",
    "FROM Employees e\n",
    "LEFT JOIN Salaries S\n",
    "    USING (employee_id)\n",
    "WHERE salary IS NULL\n",
    "UNION ALL\n",
    "SELECT employee_id\n",
    "FROM Employees e\n",
    "RIGHT JOIN Salaries S\n",
    "    USING (employee_id)\n",
    "WHERE name IS NULL\n",
    "ORDER BY employee_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38418d09",
   "metadata": {},
   "source": [
    "### 1327. List the Products Ordered in a Period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b282bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark_df_reader('Leetcode_Q_1327', ['Products', 'Orders'])\n",
    "temporary_view_registrator(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "777fb432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n",
      "|      product_name|unit|\n",
      "+------------------+----+\n",
      "|      Leetcode Kit| 100|\n",
      "|Leetcode Solutions| 130|\n",
      "+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT product_name, SUM(unit) AS unit\n",
    "FROM Products p\n",
    "JOIN Orders o\n",
    "    USING (product_id) \n",
    "WHERE MONTH(order_date) = 2 AND YEAR(order_date) = 2020\n",
    "GROUP BY product_name\n",
    "HAVING SUM(unit) >= 100 \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842aa33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
